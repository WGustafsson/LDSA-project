{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Spark setup\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import hdf5_getters\n",
    "from pyspark.sql.types import FloatType, IntegerType, StructField, StructType\n",
    "\n",
    "from tools import setup_spark_config\n",
    "\n",
    "sc, spark = setup_spark_config(\"Exploing Million Song Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Artist: Casual, Song: I Didn't Mean To\n"
     ]
    }
   ],
   "source": [
    "# Inspect some sample data\n",
    "h5 = hdf5_getters.open_h5_file_read('MillionSongSubset/A/A/A/TRAAAAW128F429D538.h5')\n",
    "\n",
    "# in byte format --> decode to string\n",
    "print('Sample Artist: %s, Song: %s' % \\\n",
    "      (hdf5_getters.get_artist_name(h5).decode('UTF-8'), hdf5_getters.get_title(h5).decode('UTF-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all subdirs\n",
    "def get_subdirs(basedir):\n",
    "    subdirs = []\n",
    "    for subdir in next(os.walk(basedir))[1]:\n",
    "        subdirs.append(os.path.join(basedir, subdir))\n",
    "    return subdirs\n",
    "\n",
    "basedir = 'MillionSongSubset'\n",
    "subdirs_rdd = sc.parallelize(get_subdirs(basedir))\n",
    "subsubdirs_rdd = subdirs_rdd.map(lambda subdir: get_subdirs(subdir)).flatMap(lambda x: x)\n",
    "subsubsubdirs_rdd = subsubdirs_rdd.map(lambda subsubdir: get_subdirs(subsubdir)).flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "894 dirs in dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['MillionSongSubset/A/R/R',\n",
       " 'MillionSongSubset/A/R/U',\n",
       " 'MillionSongSubset/A/R/I',\n",
       " 'MillionSongSubset/A/R/N',\n",
       " 'MillionSongSubset/A/R/G']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('%d dirs in dataset' % (subsubsubdirs_rdd.count()))\n",
    "subsubsubdirs_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate & get all files (songs)\n",
    "def count_and_get_files(basedir, ext='.h5'):\n",
    "    # modified version of: https://labrosa.ee.columbia.edu/millionsong/pages/iterate-over-all-songs\n",
    "    cnt = 0\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(basedir):\n",
    "        files = glob.glob(os.path.join(root,'*'+ext))\n",
    "        for file in files:\n",
    "            all_files.append(file)\n",
    "        cnt += len(files)\n",
    "    return cnt, all_files\n",
    "\n",
    "file_names_rdd = subsubsubdirs_rdd.map(lambda subsubsubdir: count_and_get_files(subsubsubdir)[1]).flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 files in dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['MillionSongSubset/A/R/R/TRARRZU128F4253CA2.h5',\n",
       " 'MillionSongSubset/A/R/R/TRARRJL128F92DED0E.h5',\n",
       " 'MillionSongSubset/A/R/R/TRARRUZ128F9307C57.h5',\n",
       " 'MillionSongSubset/A/R/R/TRARRWA128F42A0195.h5',\n",
       " 'MillionSongSubset/A/R/R/TRARRPG12903CD1DE9.h5']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('%d files in dataset' % (file_names_rdd.count()))\n",
    "file_names_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all files in rdd\n",
    "files_rdd = file_names_rdd.map(lambda filename: hdf5_getters.open_h5_file_read(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get artist name for each song\n",
    "def get_artist_name(file):\n",
    "    return hdf5_getters.get_artist_name(file).decode('UTF-8')\n",
    "\n",
    "artist_names_rdd = files_rdd.map(get_artist_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Raphaël',\n",
       " 'Julie Zenatti',\n",
       " 'The Baltimore Consort',\n",
       " 'I Hate Sally',\n",
       " 'Orlando Pops Orchestra']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artist_names_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all artists and songs in format: (artist, song)\n",
    "def get_artist_and_song(file):\n",
    "    artist = hdf5_getters.get_artist_name(file).decode('UTF-8')\n",
    "    song = hdf5_getters.get_title(file).decode('UTF-8')\n",
    "    return artist, song\n",
    "\n",
    "artist_song_rdd = files_rdd.map(lambda x: get_artist_and_song(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group songs by their artist in format: (artist, [song1, song2, song3...])\n",
    "grouped_artist_song_rdd = artist_song_rdd.groupByKey().mapValues(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4412 unique artists in dataset\n",
      "\n",
      "Rauni Pekkala has 1 songs:\n",
      "- Miesten tähden\n",
      "\n",
      "Weezer has 4 songs:\n",
      "- Dope Nose\n",
      "- I Don't Want To Let You Go\n",
      "- Don't Let Go\n",
      "- Mykel And Carli\n",
      "\n",
      "Tommy McLain has 1 songs:\n",
      "- Let It Be Me\n",
      "\n",
      "Tina Ann has 3 songs:\n",
      "- Too Late (Orange Factory Extended Mix)\n",
      "- I Do (Chris The Greek Remix)\n",
      "- Rules of Attraction (Chris Cox)\n",
      "\n",
      "Los Indios Tabajaras has 1 songs:\n",
      "- Sueño Salvaje\n"
     ]
    }
   ],
   "source": [
    "n_unique_artists = grouped_artist_song_rdd.count()\n",
    "print('%d unique artists in dataset' % (n_unique_artists))\n",
    "\n",
    "for artist_songs in grouped_artist_song_rdd.take(5):\n",
    "    print('\\n%s has %d songs:' % (artist_songs[0], len(artist_songs[1])))\n",
    "    songs = ''\n",
    "    for song in artist_songs[1]:\n",
    "        print('- %s' % (song))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 artists have no songs\n"
     ]
    }
   ],
   "source": [
    "# check if any artist has no songs -- shouldn't be possible\n",
    "print('%d artists have no songs' % (grouped_artist_song_rdd.filter(lambda x: len(x[1]) < 1).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get song data to use for analysis\n",
    "def get_song_data(file):\n",
    "    danceability = float(hdf5_getters.get_danceability(file))\n",
    "    energy = float(hdf5_getters.get_energy(file))\n",
    "    loudness = float(hdf5_getters.get_loudness(file))\n",
    "    hotness = float(hdf5_getters.get_song_hotttnesss(file))\n",
    "    year = int(hdf5_getters.get_year(file))\n",
    "    return danceability, energy, loudness, hotness, year\n",
    "\n",
    "songs_rdd = files_rdd.map(get_song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0, -9.636, 0.5479529419800353, 2008),\n",
       " (0.0, 0.0, -11.061, 0.47563846801023907, 2004),\n",
       " (0.0, 0.0, -24.14, nan, 0),\n",
       " (0.0, 0.0, -5.795, nan, 2007),\n",
       " (0.0, 0.0, -16.477, nan, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move song data from RDD to DF & table view for optimization & Spark-SQL queries\n",
    "fields = [StructField(\"danceability\", FloatType()), \\\n",
    "          StructField(\"energy\", FloatType()), \\\n",
    "          StructField(\"loudness\", FloatType()), \\\n",
    "          StructField(\"hotness\", FloatType()), \\\n",
    "          StructField(\"year\", IntegerType())]\n",
    "\n",
    "schema = StructType(fields)\n",
    "\n",
    "songs_df = spark.createDataFrame(songs_rdd, schema)\n",
    "songs_df.createOrReplaceTempView(\"songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+--------+----------+----+\n",
      "|danceability|energy|loudness|   hotness|year|\n",
      "+------------+------+--------+----------+----+\n",
      "|         0.0|   0.0|  -9.636|0.54795295|2008|\n",
      "|         0.0|   0.0| -11.061|0.47563848|2004|\n",
      "|         0.0|   0.0|  -24.14|       NaN|   0|\n",
      "|         0.0|   0.0|  -5.795|       NaN|2007|\n",
      "|         0.0|   0.0| -16.477|       NaN|   0|\n",
      "|         0.0|   0.0| -12.474|0.44545454|   0|\n",
      "|         0.0|   0.0|  -4.393|0.32773668|   0|\n",
      "|         0.0|   0.0|   -5.05|       NaN|   0|\n",
      "|         0.0|   0.0|  -4.264| 0.7883882|1982|\n",
      "|         0.0|   0.0| -13.885|       NaN|1998|\n",
      "|         0.0|   0.0|  -4.707|  0.681092|2004|\n",
      "|         0.0|   0.0|  -4.523|0.40148672|2005|\n",
      "|         0.0|   0.0|  -4.076| 0.6878737|2004|\n",
      "|         0.0|   0.0| -19.293| 0.2669552|   0|\n",
      "|         0.0|   0.0|  -3.312|0.35528553|2001|\n",
      "|         0.0|   0.0|  -6.619|0.54352427|   0|\n",
      "|         0.0|   0.0| -25.651|0.21508032|1982|\n",
      "|         0.0|   0.0|  -6.052|0.87222904|2000|\n",
      "|         0.0|   0.0|  -7.164|       NaN|   0|\n",
      "|         0.0|   0.0|  -5.704|       NaN|   0|\n",
      "+------------+------+--------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out songs with NaN values and no year\n",
    "filtered_songs_df = spark.sql(\"SELECT * FROM songs WHERE \\\n",
    "                                  isNaN(danceability) = false AND \\\n",
    "                                  isNaN(energy) = false AND \\\n",
    "                                  isNaN(hotness) = false AND \\\n",
    "                                  isNaN(year) = false AND \\\n",
    "                                  year > 0\")\n",
    "filtered_songs_df.createOrReplaceTempView(\"songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+--------+----------+----+\n",
      "|danceability|energy|loudness|   hotness|year|\n",
      "+------------+------+--------+----------+----+\n",
      "|         0.0|   0.0|  -9.636|0.54795295|2008|\n",
      "|         0.0|   0.0| -11.061|0.47563848|2004|\n",
      "|         0.0|   0.0|  -4.264| 0.7883882|1982|\n",
      "|         0.0|   0.0|  -4.707|  0.681092|2004|\n",
      "|         0.0|   0.0|  -4.523|0.40148672|2005|\n",
      "|         0.0|   0.0|  -4.076| 0.6878737|2004|\n",
      "|         0.0|   0.0|  -3.312|0.35528553|2001|\n",
      "|         0.0|   0.0| -25.651|0.21508032|1982|\n",
      "|         0.0|   0.0|  -6.052|0.87222904|2000|\n",
      "|         0.0|   0.0| -15.433| 0.5968407|1981|\n",
      "|         0.0|   0.0|  -4.325| 0.6248335|2007|\n",
      "|         0.0|   0.0|  -5.193|0.42744657|2008|\n",
      "|         0.0|   0.0|  -6.712|       0.0|2004|\n",
      "|         0.0|   0.0|   -4.13| 0.4871122|2007|\n",
      "|         0.0|   0.0|  -7.687|0.28848165|1978|\n",
      "|         0.0|   0.0|  -7.687| 0.5675917|1995|\n",
      "|         0.0|   0.0|  -21.82|0.50403434|2000|\n",
      "|         0.0|   0.0|  -5.548|    0.5764|2005|\n",
      "|         0.0|   0.0|  -7.057| 0.4764352|2007|\n",
      "|         0.0|   0.0| -13.845| 0.2998775|1986|\n",
      "+------------+------+--------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_songs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songs to parquet (better than CSV)\n",
    "dst_dir = 'parsed_songs'\n",
    "if os.path.isdir(dst_dir):\n",
    "    shutil.rmtree(dst_dir)\n",
    "filtered_songs_df.write.parquet('parsed_songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
